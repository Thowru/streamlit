# -*- coding: utf-8 -*-
"""part2_1차시도.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10yZp_n8vLbigMl-s-7MlrQVM2IWyYHU8

# 2. 데이터 준비

### 기본 라이브러리 Import
"""

# 기본라이브러리 import
from google.colab import drive
import os, json, pickle
import re

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN

import warnings
warnings.filterwarnings(action='ignore')

from google.colab import drive
drive.mount('/content/drive')

"""### 데이터 로드"""

# 구글 드라이브 mount
drive.mount('/content/gdrive')

# 데이터 파일 위치
colab_path = "gdrive/My Drive/Colab/part2"

# 데이터 파일 로드
df_orig = pd.read_csv(colab_path + "/data/train.csv")
df_orig

"""### Entity 정의"""

# Host를 UEBA의 Entity로 인식 -> Entity의 Behavior
df_orig['Host'].nunique()

# Host가 Null인 값 존재여부 확인
df_orig['Host'].isnull().sum()

# Host가 Null인 로그 확인 (랜덤추출, 여러번 실행해보며 확인)
df_orig[df_orig['Host'].isnull()].sample(10)

# Host가 Null인 값 시간 순서로 살펴보기
df_orig[df_orig['Host'].isnull()].head(20)

#157번 로그 UA 확인
df_orig.iloc[157,:]['UA']

#157과 동일한 UA 사용하는 Entity의 Behavior 확인
df_orig[df_orig['UA']=='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36 Edg/85.0.564.41'].head(10)

# index 663근처 로그 검토
df_orig.iloc[660:670, :]  #==> Host가 null이 아니어도, IP주소값이 아닌 데이터를 포함

# Host에 도메인 정보도 포함되어 있을지 확인
df_orig[df_orig['Host'].str.contains("com")==True]

# IPv4 형식의 Host만을 Entity로 식별
pat = re.compile(r"(\d+)[.](\d+)[.](\d+)[.](\d+)")  #IP 패턴탐지 정규표현식
df_train = df_orig[df_orig['Host'].str.match(pat)==True]
print("전체데이터셋: {}건,\n정상적인 Host를 가지는 데이터셋: {}건,\n학습사용 데이터: {}%".format(len(df_orig), len(df_train), round(len(df_train)/len(df_orig)*100,2)))

# 정제된 DataFrame상에서 등장한 모든 Host를 Entity로 도출한다.
df_entity = pd.DataFrame({"entity":list(df_train['Host'].unique())})
df_entity = df_entity.set_index('entity')
df_entity
# host = Entity

#df_entity_100 = df_entity[0:100]
# 속도를 위해 100개의 IP에 대해서만 Feature 추출
df_entity_100 = df_entity[:100]
df_entity_100

"""Feature Engineering

### Method
"""

# Method 데이터 패턴 분석
df_train['Method'].value_counts(dropna=False)

# 사용한 Method의 수
df_entity_100['method_cnt'] = 0
#method_cnt라는 이름의 새로운 컬럼을 'df_entity_100'데이터프레임에 추가하고, 초기값을 0을로 설정
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]
    #df_train에서 host열 값이 현재 entity와 일치하는 행들을 선택하여 group변수에 할당
    method_cnt = group['Method'].nunique()
    df_entity_100.loc[entity, 'method_cnt'] = method_cnt
    #method_cnt 생성

df_entity_100.head()

df_train[df_train['Host']=='101.224.32.28']

# 사용한 Method중 Post의 비율
df_entity_100['method_post'] = 0
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]
    method_post_percent = len(group[group['Method']=='POST']) / float(len(group))
    df_entity_100.loc[entity,'method_post'] = method_post_percent

df_entity_100.head()

"""### Protocol"""

# Protocol 데이터 패턴 분석
df_train['Protocol'].value_counts(dropna=False)

# Protocol 1.0 사용여부 (한번이라도 1.0 프로토콜 사용한 적이 있으면 True)
df_entity_100['protocol_1_0'] = None
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]
    use_1_0 = True if len(group[group['Protocol']=='HTTP/1.0']) > 0 else False
    df_entity_100.loc[entity, 'protocol_1_0'] = use_1_0

df_entity_100.head()

"""### Status"""

# Status 데이터 패턴 분석
df_train['Status'].value_counts(dropna=False)

# 정상 (200, 301, 302) Status비율
df_entity_100['status_major'] = 0
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]
    status_major_percent = len(group[group['Status'].isin(['200','301','302'])])

# 404 Status비율
df_entity_100['status_404'] = 0
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]

# 499 Status 포함여부
df_entity_100['status_499'] = 0
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]

# Status 종류 (Scanner 등은 Status가 다양할 수 있음)
df_entity_100['status_cnt'] = 0
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]

df_entity_100.head()

"""### Referer"""

# Referer 데이터 패턴 분석
df_train['Referer'].value_counts(dropna=False)

"""### Path"""

# Path 데이터 패턴 분석
df_train['Path'].value_counts(dropna=False)[:20]

# 같은 Path만 반복적으로 접근한다면?
df_entity_100['path_same'] = 0
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]

# /xmlrpc.php 접근 비율
df_entity_100['path_xmlrpc'] = 0
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]

df_entity_100.head()

"""### User Agent"""

# User Agent 데이터 패턴 분석
df_train['UA'].value_counts(dropna=False)

# Black List agent를 정의하기에는 무리가 있을 수 있음.
# User agent를 바꾸는 경우 -> 웹 스크래핑, 내용 접근 우회,DDos 공격, 스패밍 등
df_entity_100['ua_cnt'] = 0
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]

df_entity_100.head()

"""### Payload"""

# Payload 데이터 패턴 분석
df_train['Payload'].value_counts(dropna=False)

# Payload가 존재하는 경우 (XMLRPC 취약점 공격이거나 Credential 정보)
df_entity_100['has_payload'] = False
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]

df_entity_100.head()

"""### Bytes"""

# Bytes 패턴 분석
df_train['Bytes'].describe()

# Bytes의 평균 ==> 학습 Feature로 사용할지 여부 추후 판단
df_entity_100['bytes_avg'] = 0
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]

# Bytes의 분산 (다양한 URL: 정상사용자는 분산이 큼)
# 단일 접속의 경우도 분산은 작음, Scrapping의 경우 분산이 클 수 있음 ==> 학습 Feature로 사용할지 여부 추후 판단
df_entity_100['bytes_std'] = 0
for entity in df_entity_100.index.values:
    group = df_train[df_train['Host']==entity]

df_entity_100.head()

df_entity_100['bytes_std'].min()

df_entity_100[df_entity_100['bytes_std']==df_entity_100['bytes_std'].min()]

df_train[df_train['Host']=='234.80.10.45'] #전체 데이터셋이 아닌 100개 샘플이어서 절대적이라고 할 수는 없지만 Feature로 사용하기에 부적절할 수 있음

"""# 3-1. 분석용 데이터 준비: 전처리"""

# Feature Engineering Refactoring
# 일괄 처리를 위한 함수화
def feature_extract(df):
    df['method_cnt'] = 0.0
    df['method_post'] = 0.0
    df['protocol_1_0'] = False
    df['status_major'] = 0.0
    df['status_404'] = 0.0
    df['status_499'] = False
    df['status_cnt'] = 0.0
    df['path_same'] = 0.0
    df['path_xmlrpc'] = True
    df['ua_cnt'] = 0.0
    df['has_payload'] = False
    df['bytes_avg'] = 0.0
    df['bytes_std'] = 0.0
    cnt = 0

    for entity in df.index.values:
        if cnt % 500 == 0:  #진행과정 파악을 위한 로그
            print(cnt)

        group = df_train[df_train['Host']==entity]

        # 사용한 Method의 수
        method_cnt = group['Method'].nunique()
        df.loc[entity, 'method_cnt'] = method_cnt

        # 사용한 Method중 Post의 비율
        method_post_percent = len(group[group['Method']=='POST']) / float(len(group))
        df.loc[entity, 'method_post'] = method_post_percent

        # Protocol 1.0 사용여부
        use_1_0 = True if len(group[group['Protocol']=='HTTP/1.0']) > 0 else False
        df.loc[entity, 'protocol_1_0'] = use_1_0

        # 정상(200, 301, 302) Status 비율
        status_major_percent = len(group[group['Status'].isin(['200', '301', '302'])]) / float(len(group))
        df.loc[entity, 'status_major'] = status_major_percent

        # 404 Status 비율
        status_404_percent = len(group[group['Status'].isin(['404'])]) / float(len(group))
        df.loc[entity, 'status_404'] = status_404_percent

        # 499 Status 포함여부
        has_499 = True if len(group[group['Status']=='499']) > 0 else False
        df.loc[entity, 'status_499'] = has_499

        # Status 종류 (Scanner 등은 Status가 다양할 수 있음)
        status_cnt = group['Status'].nunique()
        df.loc[entity, 'status_cnt'] = status_cnt

        # 같은 Path 반복적 접근 여부
        top1_path_cnt = group['Path'].value_counts()[0]
        df.loc[entity, 'path_same'] = float(top1_path_cnt / len(group))

        # /xmlrpc.php 접근 비율
        path_xmlrpc = len(group[group['Path'].str.contains('xmlrpc.php')==True]) / float(len(group))
        df.loc[entity, 'path_xmlrpc'] = path_xmlrpc

        # User agent를 바꾸는 경우
        df.loc[entity, 'ua_cnt'] = group['UA'].nunique()

        # Payload가 존재하는 경우
        has_payload = []
        if len(group[group['Payload'] != '-']) > 0:
            has_payload.append(True)
        else:
            has_payload.append(False)
        df.loc[entity, 'has_payload'] = has_payload

        # Bytes의 평균 / 분산
        df.loc[entity, 'bytes_avg'] = np.mean(group['Bytes'])
        df.loc[entity, 'bytes_std'] = np.std(group['Bytes'])

        cnt = cnt + 1
    return df

# Commented out IPython magic to ensure Python compatibility.
# # 전체 데이터셋(HOST가 정상인 데이터)에 대해 Feature Extraction 적용
# %%time
# df_entity = feature_extract(df_entity)

# Feature 추출한 결과를 스토리지에 저장 (추출하는데 시간이 오래 소요되므로)
df_entity.to_csv(colab_path + "/data_with_feature_csv/train_processed.csv")

# 저장한 데이터 파일 로드
df_entity = pd.read_csv(colab_path + "/data_with_feature_csv/train_processed.csv", index_col='entity')

df_entity

"""### 전처리 - 결측치 처리"""

# 결측치 확인
df_entity.isnull().sum()

"""### 전처리 - 이상값 처리"""

# BoxPlot으로 확인
# True, False 인 Feature 시각화 제외
# 본 실습의 경우 이상값이 있는게 정상 => 이상값을 ML로 찾아내는게 목표
plt.figure(figsize=(15,15))
cols = ['method_cnt','method_post','status_major','status_404','status_cnt','path_same','ua_cnt','bytes_avg','bytes_std']
for i in range(len(cols)):
    plt.subplot(3, 3, i+1) # 3행 3열의 9개의 그래프 중 (i+1)번째로 state 이동
    plt.boxplot([df_entity[cols[i]]])
    plt.xticks([1],[cols[i]])

# 데이터 정규화
columns_to_scale = ['method_cnt', 'status_cnt', 'ua_cnt', 'bytes_avg', 'bytes_std']
scaler = preprocessing.MinMaxScaler()
scaler = scaler.fit(df_entity[columns_to_scale])
df_entity[columns_to_scale] = scaler.transform(df_entity[columns_to_scale])
df_entity

"""# 3-3. 탐색적 분석 : 산포도 분석"""

# scatter plot (2d)
df_entity.plot.scatter(x='method_post', y='status_404', alpha=0.5)

# scatter plot (3d)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')  # 3D 축을 추가합니다.
ax.scatter(df_entity['method_post'], df_entity['status_404'], df_entity['ua_cnt'])
ax.set_xlabel('method_post')
ax.set_ylabel('status_404')
ax.set_zlabel('ua_cnt')

"""# 3-4. 모델링"""

# 비지도 학습의 경우 모든 데이터가 학습에 사용되므로 훈련용/검증용/테스트용 데이터셋을 분리하지 않는다.
# 모델1 학습 및 평가 : K-means
cols_to_train = ['method_cnt','method_post','protocol_1_0','status_major','status_404','status_499','status_cnt','path_same','path_xmlrpc','ua_cnt','has_payload','bytes_avg','bytes_std']
modell = KMeans (n_clusters=2, random_state=42)
# 정상/ 비정상 클러스터로 나누어 보기
modell.fit(df_entity[cols_to_train])

print("각 클러스터 중심점 좌표: ", modell.cluster_centers_)
print("각 샘플의 label: ", modell.labels_)
print("중심으로부터의 거리의 합=inertia: ", modell.inertia_)
print("반복 횟수: ", modell.n_iter_)
print("fit될때까지 살펴본 feature의 수: ",modell.n_features_in_)

# 학습 결과 확인
plt.scatter(df_entity.iloc[:, 1], df_entity.iloc[:, 4], c=modell.labels_, s=60)
plt.scatter(modell.cluster_centers_[:, 1], modell.cluster_centers_[:, 4], c='red', marker='^', s=100)
plt.xlabel("method_post")
plt.ylabel("status_404")

# Predict를 통해 클러스터 할당
df_entity['cluster_kmeans'] = modell.predict(df_entity[cols_to_train])

# Outlier 클러스터에 속한 데이터 포인트 수 확인
df_entity['cluster_kmeans'].value_counts()

# Outlier 클러스터에 속한 Host 확인
df_entity[df_entity['cluster_kmeans']==0].index

# Outlier 클러스터에 속한 Host 행동 확인
df_train[df_train['Host']=='188.45.31.40']

# 모델2 학습 및 평가
model2 = DBSCAN(eps=0.5,min_samples=2)
model2.fit(df_entity[cols_to_train])

# 학습된 모델 attribute 확인
print ("Core sample의 인덱스: ", model2.core_sample_indices_)
print ("각 샘플의 label: ", model2.labels_)
print ("fit될때까지 살펴본 feature의 수: ", model2.n_features_in_)

# 밀도기반으로 스스로 찾아낸 클러스터 확인 (DBSCAN의 경우 -1이 아웃라이어)
list(set(model2.labels_))

# 학습 결과 확인
plt.scatter(df_entity.iloc[:, 1], df_entity.iloc[:, 4], c=model2.labels_, s=60)
plt.xlabel("method_post")
plt.ylabel("status_404")

# Predict를 통해 클러스터 할당
# 클러스터 할당을 위해 predict 메서드를 사용합니다
df_entity['cluster_dbscan'] = model2.fit_predict(df_entity[cols_to_train])

# Outlier 클러스터에 속한 데이터 포인트 수 확인
df_entity['cluster_dbscan'].value_counts()

# Outlier 클러스터에 속한 Host 확인
df_entity[df_entity['cluster_dbscan']!=0].index

# K-means 알고리즘에 의해서는 정상으로 판별되고, DBSCAN에서는 Outlier로 판별한 Host 확인
df_entity[(df_entity['cluster_kmeans']==1) & (df_entity['cluster_dbscan']!=0)].index

df_train[df_train['Host']=='231.211.11.16']

# K-means 클러스터링 결과를 기반으로 이상치로 판단된 Host 추출
outlier_cluster = 0  # K-means 클러스터링 결과에서 이상치로 판단된 클러스터의 라벨
outlier_hosts = df_entity[df_entity['cluster_kmeans'] == outlier_cluster].index

# 이상치로 판단된 Host의 행동 시각화 (예시로 method_post vs. status_404)
plt.figure(figsize=(10, 6))
plt.scatter(df_entity['method_post'], df_entity['status_404'], c='blue', label='Normal')
plt.scatter(df_entity.loc[outlier_hosts, 'method_post'], df_entity.loc[outlier_hosts, 'status_404'], c='red', label='Outliers')
plt.xlabel('method_post')
plt.ylabel('status_404')
plt.legend()
plt.title('K-means Outliers Visualization')
plt.show()

from sklearn.decomposition import PCA

# PCA를 사용하여 데이터의 차원을 2로 축소
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_entity[cols_to_train])

# PCA 결과를 데이터프레임에 추가
df_entity['pca_1'] = pca_result[:, 0]
df_entity['pca_2'] = pca_result[:, 1]

# 2D PCA 결과를 시각화
plt.figure(figsize=(10, 6))
plt.scatter(df_entity['pca_1'], df_entity['pca_2'], c=df_entity['cluster_kmeans'], cmap='viridis', s=60)
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.title("전체 Feature를 이용한 이상탐지된 Entity 시각화 (PCA 결과)")
plt.colorbar(label='클러스터')
plt.show()

# Features chosen for the model

chosen_data = df_entity[['method_cnt','method_post','protocol_1_0','status_major','status_404','status_499','status_cnt','path_same','path_xmlrpc','ua_cnt','has_payload','bytes_avg','bytes_std']]

min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(chosen_data)
chosen_data = pd.DataFrame(np_scaled, columns=chosen_data.columns)

n_cluster = range(1, 15)
kmeans = [KMeans(n_clusters=i, random_state=42).fit(chosen_data) for i in n_cluster]
scores = [kmeans[i].score(chosen_data) for i in range(len(kmeans))]
fig, ax = plt.subplots()
ax.plot(n_cluster, scores)
plt.show()

cluster_model = kmeans[2]
df_entity['cluster'] = cluster_model.predict(chosen_data)
df_entity['cluster'].value_counts()

from sklearn.manifold import TSNE


tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=500, random_state=42)
tsne_results = tsne.fit_transform(chosen_data)
df_entity['tsne-2d-one'] = tsne_results[:,0]
df_entity['tsne-2d-two'] = tsne_results[:,1]

tsne_cluster = df_entity.groupby('cluster').agg({'tsne-2d-one':'mean', 'tsne-2d-two':'mean'}).reset_index()

# Plotting 2D Scatterplot visualisation coloured according to cluster
import seaborn as sns

plt.figure(figsize=(20,10))

sns.scatterplot(
    x="tsne-2d-one", y="tsne-2d-two",
    data=df_entity,
    hue="cluster",
    palette=sns.color_palette("tab10", 5),
    legend="full",
    alpha=1,
    s=50
)

plt.scatter(x="tsne-2d-one", y="tsne-2d-two", data=tsne_cluster, s=10, c='b')

plt.show()

import pickle

# 클러스터링 결과에서 이상탐지된 클러스터에 속하는 entity 추출 (DBSCAN)
anomaly_entities_dbscan = df_entity[df_entity['cluster_dbscan'] != 0].index

# 추출한 entity를 pickle 파일로 저장
with open('anomaly_entities_dbscan.pkl', 'wb') as f:
    pickle.dump(anomaly_entities_dbscan, f)

# pickle 파일 불러오기
with open('anomaly_entities_dbscan.pkl', 'rb') as f:
    loaded_anomaly_entities_dbscan = pickle.load(f)

# 불러온 데이터 확인
print(loaded_anomaly_entities_dbscan)

import pickle

# 클러스터링 결과에서 이상탐지된 클러스터에 속하는 entity 추출 (K-means)
anomaly_entities_kmeans = df_entity[df_entity['cluster_kmeans'] == 0].index

# 추출한 entity를 pickle 파일로 저장
with open('anomaly_entities_kmeans.pkl', 'wb') as f:
    pickle.dump(anomaly_entities_kmeans, f)

# pickle 파일 불러오기
with open('anomaly_entities_kmeans.pkl', 'rb') as f:
    loaded_anomaly_entities_kmeans = pickle.load(f)

# 불러온 데이터 확인
print(loaded_anomaly_entities_kmeans)

import shutil

# 원하는 파일명과 경로로 이동 또는 복사
shutil.move("anomaly_entities_dbscan.pkl", colab_path)
# 또는
shutil.copy("anomaly_entities_kmeans.pkl", colab_path)

